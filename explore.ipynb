{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, argparse, warnings, sys, shutil, torch, os, numpy as np, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from preprocess import TorchOLA\n",
    "from AudioData import TrainingDataset, TrainCollate, EvalCollate, EvalDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from new_model import Net\n",
    "#from STOI import stoi\n",
    "#from PESQ import get_pesq\n",
    "from metric import get_pesq, get_stoi\n",
    "from helper_funcs import numParams, compLossMask, snr\n",
    "from criteria import mse_loss, stftm_loss\n",
    "from checkpoints import Checkpoint\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'\n",
    "\n",
    "# hyperparameter\n",
    "frame_size = 512\n",
    "overlap = 0.5\n",
    "frame_shift = int(512 * (1 - overlap))\n",
    "max_epochs = 100\n",
    "batch_size = 2\n",
    "lr_init = 64 ** (-0.5)\n",
    "eval_steps = 500\n",
    "weight_delay = 1e-7\n",
    "\n",
    "# lr scheduling\n",
    "step_num = 0\n",
    "\n",
    "warm_ups = 4000\n",
    "\n",
    "\n",
    "sr = 16000\n",
    "resume_model = None # 不是None的话 就是 相应的存model的路径\n",
    "\n",
    "model_save_path = '/media/concordia/DATA/KaiWang/pytorch_learn/pytorch_for_speech/voice_bank/Transformer/v5/checkpoints/transformer_light_0.4time.model'\n",
    "if not os.path.isdir(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "early_stop = True\n",
    "\n",
    "# file path\n",
    "train_file_list_path = '/media/concordia/DATA/KaiWang/pytorch_learn/pytorch_for_speech/voice_bank/Transformer/v1/train_file_list'\n",
    "validation_file_list_path = '/media/concordia/DATA/KaiWang/pytorch_learn/pytorch_for_speech/voice_bank/Transformer/v1/validation_file_list'\n",
    "\n",
    "# data and data_loader\n",
    "train_data = TrainingDataset(train_file_list_path, frame_size=512, frame_shift=256)\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          collate_fn=TrainCollate())\n",
    "\n",
    "validation_data = EvalDataset(validation_file_list_path, frame_size=512, frame_shift=256)\n",
    "validation_loader = DataLoader(validation_data,\n",
    "                               batch_size=1,\n",
    "                               shuffle=False,\n",
    "                               num_workers=4,\n",
    "                               collate_fn=EvalCollate())\n",
    "\n",
    "# define model\n",
    "model = Net()\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "print('Number of learnable parameters: %d' % numParams(model))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_init, weight_decay=weight_delay)\n",
    "#lr_list = [0.0002]*3 + [0.0001]*6 + [0.00005]*3 + [0.00001]*3\n",
    "\n",
    "\n",
    "time_loss = mse_loss()\n",
    "freq_loss = stftm_loss()\n",
    "\n",
    "\n",
    "def validate(net, eval_loader, test_metric=False):\n",
    "    net.eval()\n",
    "    if test_metric:\n",
    "        print('********Starting metrics evaluation on val dataset**********')\n",
    "        total_stoi = 0.0\n",
    "        total_snr = 0.0\n",
    "        total_pesq = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        count, total_eval_loss = 0, 0.0\n",
    "        for k, (features, labels) in enumerate(eval_loader):\n",
    "            features = features.cuda()  # [1, 1, num_frames, frame_size]\n",
    "            labels = labels.cuda()  # [signal_len, ]\n",
    "\n",
    "            output = net(features)  # [1, 1, sig_len_recover]\n",
    "            output = output.squeeze()  # [sig_len_recover,]\n",
    "\n",
    "            output = output[:labels.shape[-1]]  # keep length same (output label)\n",
    "\n",
    "            eval_loss = torch.mean((output - labels) ** 2)\n",
    "            total_eval_loss += eval_loss.data.item()\n",
    "\n",
    "            est_sp = output.cpu().numpy()\n",
    "            cln_raw = labels.cpu().numpy()\n",
    "            if test_metric:\n",
    "                st = get_stoi(cln_raw, est_sp, sr)\n",
    "                pe = get_pesq(cln_raw, est_sp, sr)\n",
    "                sn = snr(cln_raw, est_sp)\n",
    "                total_pesq += pe\n",
    "                total_snr += sn\n",
    "                total_stoi += st\n",
    "\n",
    "            count += 1\n",
    "        avg_eval_loss = total_eval_loss / count\n",
    "    net.train()\n",
    "    if test_metric:\n",
    "        return avg_eval_loss, total_stoi / count, total_pesq / count, total_snr / count\n",
    "    else:\n",
    "        return avg_eval_loss\n",
    "\n",
    "\n",
    "# train model\n",
    "if resume_model:\n",
    "    print('Resume model from \"%s\"' % resume_model)\n",
    "    checkpoint = Checkpoint()\n",
    "    checkpoint.load(resume_model)\n",
    "\n",
    "    start_epoch = checkpoint.start_epoch + 1\n",
    "    best_val_loss = checkpoint.best_val_loss\n",
    "    prev_val_loss = checkpoint.prev_val_loss\n",
    "    num_no_improv = checkpoint.num_no_improv\n",
    "    half_lr = checkpoint.half_lr\n",
    "    model.load_state_dict(checkpoint.state_dict)\n",
    "    optimizer.load_state_dict(checkpoint.optimizer)\n",
    "\n",
    "else:\n",
    "    print('Training from scratch.')\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    prev_val_loss = float(\"inf\")\n",
    "    num_no_improv = 0\n",
    "    half_lr = False\n",
    "\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    model.train()\n",
    "    total_train_loss, count, ave_train_loss = 0.0, 0, 0.0\n",
    "\n",
    "    '''\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr_list[epoch]\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    if half_lr:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] / 2\n",
    "            print('Learning rate adjusted to  %5f' % (param_group['lr']))\n",
    "        half_lr = False\n",
    "    '''\n",
    "\n",
    "    for index, (features, labels, sig_len) in enumerate(train_loader):\n",
    "\n",
    "        step_num += 1\n",
    "        if step_num <= warm_ups:\n",
    "            lr = 0.2 * lr_init * min(step_num ** (-0.5),\n",
    "                                     step_num * (warm_ups ** (-1.5)))\n",
    "        else:\n",
    "            lr = 0.0004 * (0.98 ** ((epoch - 1) // 2))\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            print('Learning rate adjusted to  %6f' % (param_group['lr']))\n",
    "\n",
    "\n",
    "        # feature -- [batch_size, 1, nframes, frame_size]\n",
    "        features = features.cuda()\n",
    "        # label -- [batch_size, 1, signal_length]\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        loss_mask = compLossMask(labels, nframes=sig_len)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(features)  # output -- [batch_size, 1, sig_len_recover]\n",
    "        output = output[:, :, :labels.shape[-1]]  # [batch_size, 1, sig_len]\n",
    "\n",
    "        loss_time = time_loss(output, labels, loss_mask)\n",
    "        loss_freq = freq_loss(output, labels, loss_mask)\n",
    "\n",
    "        loss = 0.4 * loss_time + 0.6 * loss_freq\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = loss.data.item()\n",
    "        total_train_loss += train_loss\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        del loss, loss_time, loss_freq, output, loss_mask, features, labels\n",
    "        print('iter = {}/{}, epoch = {}/{}, loss = {:.5f}'.format(index + 1, len(train_loader), epoch + 1, max_epochs, train_loss))\n",
    "\n",
    "        if (index + 1) % eval_steps == 0:\n",
    "            ave_train_loss = total_train_loss / count\n",
    "\n",
    "            # validation\n",
    "            avg_eval_loss = validate(model, validation_loader)\n",
    "            model.train()\n",
    "\n",
    "            print('Epoch [%d/%d], Iter [%d/%d],  ( TrainLoss: %.4f | EvalLoss: %.4f )' % (\n",
    "            epoch + 1, max_epochs, index + 1, len(train_loader), ave_train_loss, avg_eval_loss))\n",
    "\n",
    "            count = 0\n",
    "            total_train_loss = 0.0\n",
    "\n",
    "\n",
    "        if (index + 1) % len(train_loader) == 0:\n",
    "            break\n",
    "\n",
    "    # validate metric\n",
    "    avg_eval, avg_stoi, avg_pesq, avg_snr = validate(model, validation_loader, test_metric=True)\n",
    "    model.train()\n",
    "    print('#' * 50)\n",
    "    print('')\n",
    "    print('After {} epoch the performance on validation score is a s follows:'.format(epoch + 1))\n",
    "    print('')\n",
    "    print('Avg_loss: {:.4f}'.format(avg_eval))\n",
    "    print('STOI: {:.4f}'.format(avg_stoi))\n",
    "    print('SNR: {:.4f}'.format(avg_snr))\n",
    "    print('PESQ: {:.4f}'.format(avg_pesq))\n",
    "\n",
    "\n",
    "    # adjust learning rate and early stop\n",
    "    if avg_eval >= prev_val_loss:\n",
    "        num_no_improv += 1\n",
    "        #if num_no_improv == 2:\n",
    "            #half_lr = True\n",
    "        if num_no_improv >= 10 and early_stop is True:\n",
    "            print(\"No improvement and apply early stop\")\n",
    "            break\n",
    "    else:\n",
    "        num_no_improv = 0\n",
    "\n",
    "    prev_val_loss = avg_eval\n",
    "\n",
    "    if avg_eval < best_val_loss:\n",
    "        best_val_loss = avg_eval\n",
    "        is_best_model = True\n",
    "    else:\n",
    "        is_best_model = False\n",
    "\n",
    "    # save model\n",
    "    latest_model = 'latest.model'\n",
    "    best_model = 'best.model'\n",
    "\n",
    "    checkpoint = Checkpoint(start_epoch=epoch,\n",
    "                            best_val_loss=best_val_loss,\n",
    "                            prev_val_loss=prev_val_loss,\n",
    "                            state_dict=model.state_dict(),\n",
    "                            optimizer=optimizer.state_dict(),\n",
    "                            num_no_improv=num_no_improv,\n",
    "                            half_lr=half_lr)\n",
    "    checkpoint.save(is_best=is_best_model,\n",
    "                    filename=os.path.join(model_save_path, latest_model + '-{}.model'.format(epoch + 1)),\n",
    "                    best_model=os.path.join(model_save_path, best_model))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
